{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T16:15:22.163880Z",
     "start_time": "2017-06-12T16:15:22.142230"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Garrett/anaconda2/envs/py36ipy/lib/python3.6/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['copy']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T16:15:45.725494Z",
     "start_time": "2017-06-12T16:15:22.543683"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readGz(f):\n",
    "  for l in gzip.open(f):\n",
    "    yield eval(l)\n",
    "\n",
    "allHelpful = [] ## total number of helpful ratings\n",
    "userHelpful = defaultdict(list)\n",
    "\n",
    "for l in readGz(\"train.json.gz\"):\n",
    "  user,item = l['reviewerID'],l['itemID']\n",
    "  allHelpful.append(l['helpful'])\n",
    "  userHelpful[user].append(l['helpful'])\n",
    "\n",
    "averageRate = sum([x['nHelpful'] for x in allHelpful]) * 1.0 / sum([x['outOf'] for x in allHelpful])\n",
    "## average helpfulness across all users: 0.8519720886532813\n",
    "userRate = {}   ## dictionary of users and their average helpfulness. helpfulness rating is average of all users if user\n",
    "                ## has no helpfulness data on them\n",
    "for u in userHelpful:\n",
    "  totalU = sum([x['outOf'] for x in userHelpful[u]])\n",
    "  if totalU > 0:\n",
    "    userRate[u] = sum([x['nHelpful'] for x in userHelpful[u]]) * 1.0 / totalU\n",
    "  else:\n",
    "    userRate[u] = averageRate\n",
    "\n",
    "# predictions = open(\"predictions_Helpful.txt\", 'w')\n",
    "# for l in open(\"pairs_Helpful.txt\"):\n",
    "#   if l.startswith(\"userID\"):\n",
    "#     #header\n",
    "#     predictions.write(l)\n",
    "#     continue\n",
    "#   u,i,outOf = l.strip().split('-')\n",
    "#   outOf = int(outOf)\n",
    "# #   if outOf == 0:\n",
    "# #     predictions.write(u + '-' + i + '-' + str(outOf) + ',' + str(0) + '\\n')\n",
    "#   if u in userRate:\n",
    "#     predictions.write(u + '-' + i + '-' + str(outOf) + ',' + str(outOf*userRate[u]) + '\\n')\n",
    "#   else:\n",
    "#     predictions.write(u + '-' + i + '-' + str(outOf) + ',' + str(outOf*averageRate) + '\\n')\n",
    "\n",
    "# predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating syllable counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T16:15:47.069209Z",
     "start_time": "2017-06-12T16:15:45.727513"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import cmudict\n",
    "d = cmudict.dict() \n",
    "def nsyl(word):\n",
    "    max_syl = 0\n",
    "    if word.lower() in d:\n",
    "        for syl_group in d[word.lower()]:\n",
    "            tot_syl = 0\n",
    "            for syl in syl_group:\n",
    "                if str(syl[-1]).isdigit():\n",
    "                    tot_syl += 1\n",
    "            max_syl = max(max_syl,tot_syl)\n",
    "    return max_syl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T16:16:16.762267Z",
     "start_time": "2017-06-12T16:15:47.071122"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def readGz(f):\n",
    "  for l in gzip.open(f):\n",
    "    yield eval(l)\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "train_df = getDF('train.json.gz')\n",
    "test_df = getDF('test_Helpful.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T16:16:16.813322Z",
     "start_time": "2017-06-12T16:16:16.764471"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caps = \"([A-Z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "digits = \"([0-9])\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + caps + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + caps + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return len(sentences)\n",
    "\n",
    "# len(split_into_sentences(filtered_df.loc[14]['reviewText']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T16:16:16.823679Z",
     "start_time": "2017-06-12T16:16:16.815485"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorybinner (df):\n",
    "    newdf = df\n",
    "    newdf['cat0'] = np.where((newdf['categoryID']==0), 1, 0)\n",
    "    newdf['cat1'] = np.where((newdf['categoryID']==1), 1, 0)\n",
    "    newdf['cat2'] = np.where((newdf['categoryID']==2), 1, 0)\n",
    "    newdf['cat3'] = np.where((newdf['categoryID']==3), 1, 0)\n",
    "    newdf['cat4'] = np.where((newdf['categoryID']==4), 1, 0)\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split our training data into multiple dataframes based on how many reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T16:16:16.870334Z",
     "start_time": "2017-06-12T16:16:16.826589"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_dataframe_prepper(df):\n",
    "    df['num_helpful'] = df['helpful'].apply(lambda x: x['nHelpful'])\n",
    "    df['outof'] = df['helpful'].apply(lambda x: x['outOf'])\n",
    "    df['helpfulratio'] = df['num_helpful']/df['outof']\n",
    "    df = df[df.helpfulratio.notnull()]\n",
    "    df['num_reviews_written'] = df['reviewerID'].map(df['reviewerID'].value_counts().to_dict())\n",
    "    df['num_item_reviews'] = df['itemID'].map(df['itemID'].value_counts().to_dict())\n",
    "    df['num_words'] = df['reviewText'].apply(lambda x: len(re.findall(r'\\w+',x)))\n",
    "    df['num_sentences'] = df['reviewText'].apply(lambda x: split_into_sentences(x) if split_into_sentences(x) != 0 else 1)\n",
    "    df['num_syllables'] = df['reviewText'].apply(lambda x: sum([nsyl(b) for b in (re.findall(r'\\w+',x))]))\n",
    "    df['num_chars'] = df['reviewText'].apply(lambda x: sum([len(b) for b in (re.findall(r'\\w+',x))]))\n",
    "    df['Automated_Readability_Index'] = 4.71*df['num_chars']/df['num_words'] + 0.5*df['num_words']/df['num_sentences'] - 21.43\n",
    "    df = df.drop(['num_syllables','num_chars'],axis=1)\n",
    "    df['summ_num_words'] = df['summary'].apply(lambda x: len(re.findall(r'\\w+',x)))\n",
    "    df['conciseness'] = df['summ_num_words']/df['num_words']\n",
    "    df = df.drop(['summ_num_words','num_words'],1)\n",
    "    df = categorybinner(df)\n",
    "    df = df[(df['outof']>0)]\n",
    "    Ones_df = df[(df['outof'] ==1)]\n",
    "    Low_Outof = df[(df['outof'] <=10) & (df['outof']>=1)]\n",
    "    More_Outof = df[(df['outof']<=500)&(df['outof']>=57)]\n",
    "    Less_Outof = df[(df['outof']<57) & (df['outof']>10)]\n",
    "    \n",
    "    \n",
    "    return More_Outof, Less_Outof, Low_Outof, Ones_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T16:17:29.214267Z",
     "start_time": "2017-06-12T16:16:16.873404"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Garrett/anaconda2/envs/py36ipy/lib/python3.6/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Garrett/anaconda2/envs/py36ipy/lib/python3.6/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Garrett/anaconda2/envs/py36ipy/lib/python3.6/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Garrett/anaconda2/envs/py36ipy/lib/python3.6/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Garrett/anaconda2/envs/py36ipy/lib/python3.6/site-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Garrett/anaconda2/envs/py36ipy/lib/python3.6/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Garrett/anaconda2/envs/py36ipy/lib/python3.6/site-packages/ipykernel/__main__.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "train_df = getDF('train.json.gz')\n",
    "High_oo_df, Less_oo_df,Low_oo_df, Ones_oo_df = training_dataframe_prepper(train_df)\n",
    "Highlabels = High_oo_df['helpfulratio'].values\n",
    "Lesslabels = Less_oo_df['helpfulratio'].values\n",
    "Lowlabels = Low_oo_df['helpfulratio'].values\n",
    "Oneslabels = Ones_oo_df['helpfulratio'].values\n",
    "High_oo_df = High_oo_df.drop(['num_helpful','categoryID','categories','itemID','helpful','reviewerID',\n",
    "                'reviewText','reviewHash','reviewTime','summary',\n",
    "                'unixReviewTime','helpfulratio','price'],1)\n",
    "Less_oo_df = Less_oo_df.drop(['num_helpful','categoryID','categories','itemID','helpful','reviewerID','helpfulratio',\n",
    "                'reviewText','reviewHash','reviewTime','summary',\n",
    "                'unixReviewTime','price'],1)\n",
    "Low_oo_df = Low_oo_df.drop(['num_helpful','categoryID','categories','itemID','helpful','reviewerID','helpfulratio',\n",
    "                'reviewText','reviewHash','reviewTime','summary',\n",
    "                'unixReviewTime','price'],1)\n",
    "Ones_oo_df = Ones_oo_df.drop(['num_helpful','categoryID','categories','itemID','helpful','reviewerID','helpfulratio',\n",
    "                'reviewText','reviewHash','reviewTime','summary',\n",
    "                'unixReviewTime','price'],1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T16:17:29.242675Z",
     "start_time": "2017-06-12T16:17:29.216415"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_dataframe_prepper(df):\n",
    "\n",
    "    df['outof'] = df['helpful'].apply(lambda x: x['outOf'])\n",
    "    df = df.drop('helpful',1)\n",
    "    df = df[df['outof'] !=  0]\n",
    "\n",
    "    df['num_reviews_written'] = df['reviewerID'].map(df['reviewerID'].value_counts().to_dict())\n",
    "    df['num_item_reviews'] = df['itemID'].map(df['itemID'].value_counts().to_dict())\n",
    "    df['num_words'] = df['reviewText'].apply(lambda x: len(re.findall(r'\\w+',x)))\n",
    "    df['num_sentences'] = df['reviewText'].apply(lambda x: split_into_sentences(x) if split_into_sentences(x) != 0 else 1)\n",
    "    df['num_syllables'] = df['reviewText'].apply(lambda x: sum([nsyl(b) for b in (re.findall(r'\\w+',x))]))\n",
    "    df['num_chars'] = df['reviewText'].apply(lambda x: sum([len(b) for b in (re.findall(r'\\w+',x))]))\n",
    "    df['Automated_Readability_Index'] = 4.71*df['num_chars']/df['num_words'] + 0.5*df['num_words']/df['num_sentences'] - 21.43\n",
    "    df = df.drop(['num_syllables','num_chars'],axis=1)   \n",
    "    df['summ_num_words'] = df['summary'].apply(lambda x: len(re.findall(r'\\w+',x)))\n",
    "    df['conciseness'] = df['summ_num_words']/df['num_words']\n",
    "    df = df.drop(['summ_num_words','num_words'],1)\n",
    "    df = categorybinner(df)\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T16:17:34.216131Z",
     "start_time": "2017-06-12T16:17:29.245198"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2695\n"
     ]
    }
   ],
   "source": [
    "test_df = getDF('test_Helpful.json.gz')\n",
    "test_df = test_dataframe_prepper(test_df)\n",
    "print(test_df.price.isnull().sum())\n",
    "\n",
    "test_df = test_df.drop(['categoryID','categories',\n",
    "                            'reviewText','reviewHash','reviewTime',\n",
    "                        'summary','unixReviewTime','price'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T16:17:34.254777Z",
     "start_time": "2017-06-12T16:17:34.220228"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemID</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>rating</th>\n",
       "      <th>outof</th>\n",
       "      <th>num_reviews_written</th>\n",
       "      <th>num_item_reviews</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>Automated_Readability_Index</th>\n",
       "      <th>conciseness</th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I520932398</td>\n",
       "      <td>U816789534</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4.660000</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I149943341</td>\n",
       "      <td>U628436634</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.931304</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I909025835</td>\n",
       "      <td>U924107228</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.781891</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I408726477</td>\n",
       "      <td>U545844741</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.461429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I353318513</td>\n",
       "      <td>U264684350</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>7.321532</td>\n",
       "      <td>0.004566</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       itemID  reviewerID  rating  outof  num_reviews_written  \\\n",
       "0  I520932398  U816789534     3.0      2                    1   \n",
       "2  I149943341  U628436634     5.0      1                    1   \n",
       "3  I909025835  U924107228     5.0      1                    1   \n",
       "5  I408726477  U545844741     5.0      2                    1   \n",
       "7  I353318513  U264684350     5.0      2                    1   \n",
       "\n",
       "   num_item_reviews  num_sentences  Automated_Readability_Index  conciseness  \\\n",
       "0                 1              2                     4.660000     0.068966   \n",
       "2                 1              2                     1.931304     0.173913   \n",
       "3                 1             13                     0.781891     0.028169   \n",
       "5                 3              3                     0.461429     0.142857   \n",
       "7                 1             11                     7.321532     0.004566   \n",
       "\n",
       "   cat0  cat1  cat2  cat3  cat4  \n",
       "0     1     0     0     0     0  \n",
       "2     1     0     0     0     0  \n",
       "3     1     0     0     0     0  \n",
       "5     1     0     0     0     0  \n",
       "7     0     1     0     0     0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T16:17:34.280507Z",
     "start_time": "2017-06-12T16:17:34.257967"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>outof</th>\n",
       "      <th>num_reviews_written</th>\n",
       "      <th>num_item_reviews</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>Automated_Readability_Index</th>\n",
       "      <th>conciseness</th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5.0</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6.154583</td>\n",
       "      <td>0.116071</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>5.0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>4.317480</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>5.0</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>11.129839</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>5.0</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>3.742192</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>2.0</td>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3.655882</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rating  outof  num_reviews_written  num_item_reviews  num_sentences  \\\n",
       "37      5.0     24                    5                 3              6   \n",
       "67      5.0     11                    5                 2             14   \n",
       "75      5.0     22                    7                 9              3   \n",
       "161     5.0     22                    3                19              5   \n",
       "268     2.0     39                    6                 6              5   \n",
       "\n",
       "     Automated_Readability_Index  conciseness  cat0  cat1  cat2  cat3  cat4  \n",
       "37                      6.154583     0.116071     1     0     0     0     0  \n",
       "67                      4.317480     0.027778     1     0     0     0     0  \n",
       "75                     11.129839     0.024096     1     0     0     0     0  \n",
       "161                     3.742192     0.027397     1     0     0     0     0  \n",
       "268                     3.655882     0.044118     1     0     0     0     0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Less_oo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Grid Search to determine best parameters for Gradient Boosted Classifier for datapoint where only one review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T16:17:34.297857Z",
     "start_time": "2017-06-12T16:17:34.283716"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GBCgridsearch (df, labels):\n",
    "    xtrain, xvalid, ytrain, yvalid = train_test_split(df, labels, test_size = 0.3, random_state=np.random.randint(0,100))\n",
    "#     parameters = {'loss':['deviance','exponential'],'n_estimators':[120,140,160], 'learning_rate':[0.1,0.2], 'max_depth':[2,3,4]}\n",
    "    parameters = {'loss':['deviance'],'n_estimators':[120,140,160], 'learning_rate':[0.1,0.2], 'max_depth':[2,3,4]}\n",
    "    gbc = GradientBoostingClassifier()\n",
    "    clf = GridSearchCV(gbc, parameters, verbose=True, n_jobs=-1, refit=True)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "#     y_pred = clf.predict(xvalid)\n",
    "    train_acc = mean_absolute_error(clf.predict(xvalid), yvalid)\n",
    "    train_acc1 = mean_squared_error(clf.predict(xvalid), yvalid)\n",
    "    print ('MAE = ' + str(train_acc)) \n",
    "    print ('MSE = ' + str(train_acc1)) \n",
    "    print (clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T16:18:08.875413Z",
     "start_time": "2017-06-12T16:17:34.301051"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " outof ones\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   24.5s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:   33.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE = 0.246789633435\n",
      "MSE = 0.246789633435\n",
      "{'learning_rate': 0.2, 'loss': 'deviance', 'max_depth': 2, 'n_estimators': 120}\n"
     ]
    }
   ],
   "source": [
    "print('\\n' + ' outof ones')\n",
    "GBCgridsearch(Ones_oo_df, Oneslabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Grid Search for Gradient Boosted Regressor. Used for rest of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T16:18:08.907981Z",
     "start_time": "2017-06-12T16:18:08.879092"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GBRgridsearch (df, labels):\n",
    "    xtrain, xvalid, ytrain, yvalid = train_test_split(df, labels, test_size = 0.3, random_state=np.random.randint(0,100))\n",
    "    parameters = {'loss':['ls', 'lad', 'huber', 'quantile'],'n_estimators':[120,140,160], 'learning_rate':[0.1,0.2], 'max_depth':[3,4,7,8]}\n",
    "    gbc = GradientBoostingRegressor()\n",
    "    clf = GridSearchCV(gbc, parameters, verbose=True, n_jobs=7, refit=True)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "#     y_pred = clf.predict(xvalid)\n",
    "    train_acc = mean_absolute_error(clf.predict(xvalid), yvalid)\n",
    "    train_acc1 = mean_squared_error(clf.predict(xvalid), yvalid)\n",
    "    print ('MAE = ' + str(train_acc)) \n",
    "    print ('MSE = ' + str(train_acc1)) \n",
    "    print (clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T16:37:11.713363Z",
     "start_time": "2017-06-12T16:18:08.910875"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high outof\n",
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=7)]: Done 288 out of 288 | elapsed:   16.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE = 0.0555958769645\n",
      "MSE = 0.020090597375\n",
      "{'learning_rate': 0.1, 'loss': 'quantile', 'max_depth': 8, 'n_estimators': 160}\n",
      "\n",
      "less outof\n",
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed:   39.9s\n",
      "[Parallel(n_jobs=7)]: Done 288 out of 288 | elapsed:   59.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE = 0.0846352710911\n",
      "MSE = 0.0163670303372\n",
      "{'learning_rate': 0.1, 'loss': 'ls', 'max_depth': 3, 'n_estimators': 120}\n",
      "\n",
      "low outof\n",
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=7)]: Done 288 out of 288 | elapsed: 12.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE = 0.279986121859\n",
      "MSE = 0.119560277579\n",
      "{'learning_rate': 0.1, 'loss': 'ls', 'max_depth': 3, 'n_estimators': 120}\n",
      "\n",
      "Ones outof\n",
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:   41.4s\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=7)]: Done 288 out of 288 | elapsed:  4.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE = 0.354963537013\n",
      "MSE = 0.179051754529\n",
      "{'learning_rate': 0.1, 'loss': 'ls', 'max_depth': 3, 'n_estimators': 120}\n"
     ]
    }
   ],
   "source": [
    "print('high outof')\n",
    "GBRgridsearch(High_oo_df,Highlabels) ## 0.0417611\n",
    "print('\\n' + 'less outof')\n",
    "GBRgridsearch(Less_oo_df,Lesslabels) ## 0.0799364\n",
    "print('\\n' + 'low outof')\n",
    "GBRgridsearch(Low_oo_df, Lowlabels) ## 0.27929156\n",
    "print('\\n' + 'Ones outof')\n",
    "GBRgridsearch(Ones_oo_df,Oneslabels)## 0.3543893944"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T16:38:20.339144Z",
     "start_time": "2017-06-12T16:37:11.718920"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ablation\n",
      "Mean Absolute Error removing feature rating: 0.2877196910017685\n",
      "Mean Absolute Error removing feature outof: 0.2809878044689645\n",
      "Mean Absolute Error removing feature num_reviews_written: 0.28089869778620397\n",
      "Mean Absolute Error removing feature num_item_reviews: 0.28238237296921764\n",
      "Mean Absolute Error removing feature num_sentences: 0.2813237860588637\n",
      "Mean Absolute Error removing feature Automated_Readability_Index: 0.2804551326775394\n",
      "Mean Absolute Error removing feature conciseness: 0.280665965733452\n",
      "Mean Absolute Error removing feature cat0: 0.28084383817570696\n",
      "Mean Absolute Error removing feature cat1: 0.28072456880751895\n",
      "Mean Absolute Error removing feature cat2: 0.28090067491007326\n",
      "Mean Absolute Error removing feature cat3: 0.280938971970181\n",
      "Mean Absolute Error removing feature cat4: 0.28087892430608147\n"
     ]
    }
   ],
   "source": [
    "def skgbr(x_train, x_test, y_train, y_test):\n",
    "    LNR = GradientBoostingRegressor(learning_rate=0.1, loss='ls',max_depth=3,n_estimators=140)\n",
    "    LNR.fit(x_train, y_train)\n",
    "    LNR_pred = LNR.predict(x_test)\n",
    "    mse = mean_squared_error(y_test, LNR_pred)\n",
    "    mae = mean_absolute_error(y_test, LNR_pred)\n",
    "    return [mse,mae]\n",
    "print('ablation')\n",
    "cols = Low_oo_df.columns\n",
    "\n",
    "for i in cols:\n",
    "    dbt1 = Low_oo_df.drop(i, axis=1, inplace=False)\n",
    "    dbtx_train2, dbtx_test2, dbty_train2, dbty_test2 = train_test_split(dbt1, Lowlabels, test_size=0.1, random_state=10)\n",
    "    err1 = skgbr(dbtx_train2, dbtx_test2, dbty_train2, dbty_test2)\n",
    "    print('Mean Absolute Error removing feature {}: {}'.format(i, err1[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-09T19:18:45.755974Z",
     "start_time": "2017-06-09T19:18:37.758527"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating regressors/classifier\n"
     ]
    }
   ],
   "source": [
    "GBRhigh = GradientBoostingRegressor(learning_rate=0.1, loss='quantile',max_depth=8,n_estimators=160)\n",
    "GBRhigh.fit(High_oo_df,Highlabels)\n",
    "GBRless = GradientBoostingRegressor(learning_rate=0.1, loss='ls',max_depth=3,n_estimators=120)\n",
    "GBRless.fit(Less_oo_df,Lesslabels)\n",
    "GBRlow = GradientBoostingRegressor(learning_rate=0.1, loss='ls',max_depth=3,n_estimators=120)\n",
    "GBRlow.fit (Low_oo_df,Lowlabels)\n",
    "GBCones = GradientBoostingClassifier(learning_rate= 0.1, loss='deviance', max_depth= 2, n_estimators= 120)\n",
    "GBCones.fit(Ones_oo_df,Oneslabels)\n",
    "print('creating regressors/classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-09T19:19:13.650229Z",
     "start_time": "2017-06-09T19:18:54.721981"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Helpful_submission2.txt\", 'w')\n",
    "\n",
    "for l in open(\"pairs_Helpful.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,i,outOf = l.strip().split('-')\n",
    "    outOf = int(outOf)\n",
    "    ## WRITE A BETTER REGRESSOR FOR OUTOF LESS THAN 10\n",
    "    if outOf == 0:\n",
    "        predictions.write(u + '-' + i + '-' + str(outOf) + ',' + str(0) + '\\n')\n",
    "    elif outOf ==1:\n",
    "        newdf = test_df.loc[(test_df['itemID'] == i) & (test_df['reviewerID'] == u)].drop(['itemID','reviewerID'],1)\n",
    "        pred = GBCones.predict(newdf)[0]\n",
    "        \n",
    "        predictions.write(u + '-' + i + '-' + str(outOf) + ',' + str(pred) + '\\n')\n",
    "        \n",
    "    elif outOf >=2 and outOf <=10:\n",
    "        newdf = test_df.loc[(test_df['itemID'] == i) & (test_df['reviewerID'] == u)].drop(['itemID','reviewerID'],1)\n",
    "        pred = GBRlow.predict(newdf)[0]\n",
    "        if pred > 1:\n",
    "            pred = 1\n",
    "        pred = round(pred, 2)\n",
    "        predictions.write(u + '-' + i + '-' + str(outOf) + ',' + str(round(pred*outOf)) + '\\n')\n",
    "#         predictions.write(u + '-' + i + '-' + str(outOf) + ',' + str(float(guessings[outOf])*outOf) + '\\n')\n",
    "        \n",
    "    elif outOf <57 and outOf > 10:\n",
    "        newdf = test_df.loc[(test_df['itemID'] == i) & (test_df['reviewerID'] == u)].drop(['itemID','reviewerID'],1)\n",
    "        pred = GBRless.predict(newdf)[0]\n",
    "        if pred > 1:\n",
    "            pred = 1\n",
    "        pred = round(pred, 2)\n",
    "        predictions.write(u + '-' + i + '-' + str(outOf) + ',' + str(round(pred*outOf)) + '\\n')\n",
    "        \n",
    "    elif outOf >=57:# and outOf > 10:\n",
    "        newdf = test_df.loc[(test_df['itemID'] == i) & (test_df['reviewerID'] == u)].drop(['itemID','reviewerID'],1)\n",
    "        pred = GBRhigh.predict(newdf)[0]\n",
    "        if pred > 1:\n",
    "            pred = 1\n",
    "        pred = round(pred, 2)\n",
    "        predictions.write(u + '-' + i + '-' + str(outOf) + ',' + str(round(pred*outOf)) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
